{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7c30px5HbrW"
   },
   "source": [
    "# Building inverted index and answering queries\n",
    "\n",
    "In the first part of the lab you are going to implement a standard document processing pipeline and then build a simple search engine based on it:\n",
    "- starting from crawling documents,\n",
    "- then building an inverted index,\n",
    "- and answering queries using this index.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "First, we need a unified approach to documents and queries preprocessing. Implement a class responsible for that. Complete the code for given functions (most of them are just one-liners) and make sure you pass the tests. Make use of `nltk` library, `spacy`, or any other you know."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "den9w0x-d61x",
    "outputId": "1595d145-a5ad-4ee9-cdd7-22a32d19c530",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:06.541786Z",
     "start_time": "2024-09-17T14:55:06.532223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/a/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o2kGKev8HbrX",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:06.787486Z",
     "start_time": "2024-09-17T14:55:06.774748Z"
    }
   },
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.stop_words = {\n",
    "\t\t\t'a', 'an', 'and', 'are', 'as', 'at',\n",
    "\t\t\t'be', 'by', 'for', 'from', 'has',\n",
    "\t\t\t'he', 'in', 'is', 'it', 'its',\n",
    "\t\t\t'of', 'on', 'that', 'the', 'to',\n",
    "\t\t\t'was', 'were', 'will', 'with'\n",
    "\t\t}\n",
    "\t\tself.ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\t#TODO word tokenize text using nltk lib\n",
    "\t\ttokens = nltk.word_tokenize(text)\n",
    "\t\treturn tokens\n",
    "\t\t# return ['one', 'two', 'three']\n",
    "\n",
    "\tdef stem(self, word, stemmer):\n",
    "\t\t#TODO stem word using provided stemmer\n",
    "\t\tstem = stemmer.stem(word)\n",
    "\t\treturn stem\n",
    "\t\t# return 'stemmed_word'\n",
    "\n",
    "\tdef is_apt_word(self, word):\n",
    "\t\t#TODO check if word is appropriate - not a stop word and isalpha,\n",
    "\t\t# i.e consists of letters, not punctuation, numbers, dates\n",
    "\t\tapt = word.isalpha() and (word not in self.stop_words)\n",
    "\t\treturn apt\n",
    "\n",
    "\tdef preprocess(self, text):\n",
    "\t\t# and stem it, ignoring not appropriate words\n",
    "\t\ttokens = self.tokenize(text.lower())\n",
    "\t\ttokens = [self.stem(token, self.ps) for token in tokens if self.is_apt_word(token)]\n",
    "\t\treturn tokens\n",
    "\t\t# return ['one', 'two', 'three']"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lNkN9IfHbrY"
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZbcaEo8ZHbrY",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:07.030957Z",
     "start_time": "2024-09-17T14:55:06.842533Z"
    }
   },
   "source": [
    "prep = Preprocessor()\n",
    "text = 'To be, or not to be, that is the question'\n",
    "\n",
    "assert prep.tokenize(text) == ['To', 'be', ',', 'or', 'not', 'to', 'be', ',', 'that', 'is', 'the', 'question']\n",
    "assert prep.stem('retrieval', prep.ps) == 'retriev'\n",
    "assert prep.is_apt_word('qwerty123') is False\n",
    "assert prep.preprocess(text) == ['or', 'not', 'question']"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA8EoPQcHbrY"
   },
   "source": [
    "## Crawling and Indexing\n",
    "\n",
    "### Base classes\n",
    "\n",
    "Here are some base classes you will need for writing an indexer. The code is given, still, you need to change some. Namely, the `parse` method (it is also possible to use your own implementations of other methods, just make sure they work). The reason to change is that the method always makes complete parsing, which we want to avoid, when we only need e.g. links or a specific portions of text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ORTuCvAHbrY",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:07.127614Z",
     "start_time": "2024-09-17T14:55:07.034353Z"
    }
   },
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "\n",
    "class Document:\n",
    "\n",
    "\tdef __init__(self, url):\n",
    "\t\tself.url = url\n",
    "\n",
    "\tdef download(self):\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(self.url)\n",
    "\t\t\tif response.status_code == 200:\n",
    "\t\t\t\tself.content = response.content\n",
    "\t\t\t\treturn True\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn False\n",
    "\t\texcept:\n",
    "\t\t\treturn False\n",
    "\n",
    "\tdef persist(self, path):\n",
    "\t\t# this code is not supposed to be good :)\n",
    "\t\t# Please discuss why this line is bad\n",
    "\t\twith open(os.path.join(path, quote(self.url).replace('/', '_')), 'wb') as f:\n",
    "\t\t\tf.write(self.content)\n",
    "\n",
    "\n",
    "class HtmlnbcnewsArticle(Document):\n",
    "\n",
    "\tdef normalize(self, href):\n",
    "\t\tif href is not None and href[:4] != 'http':\n",
    "\t\t\thref = urllib.parse.urljoin(self.url, href)\n",
    "\t\treturn href\n",
    "\n",
    "\tdef parse(self):\n",
    "\t\t#TODO change this method\n",
    "\n",
    "\t\tdef tag_visible(element):\n",
    "\t\t\tif element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "\t\t\t\treturn False\n",
    "\t\t\tif isinstance(element, Comment):\n",
    "\t\t\t\treturn False\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\tmodel = BeautifulSoup(self.content, \"html.parser\")\n",
    "\n",
    "\t\tself.anchors = []\n",
    "\t\ta = model.find_all('a')\n",
    "\t\tfor anchor in a:\n",
    "\t\t\thref = self.normalize(anchor.get('href'))\n",
    "\t\t\ttext = anchor.text\n",
    "\t\t\tself.anchors.append((text, href))\n",
    "\n",
    "\t\t# extract only header and main content\n",
    "\t\t# discuss why using classes like article-body__content__17Yit\n",
    "\t\t# is the wrong strategy\n",
    "\t\t\n",
    "\t\t# header = model.select_one(\"h1[class *= 'headline']\")\n",
    "\t\t# content = model.select_one(\"div[class *= 'article-body']\")\n",
    "\n",
    "\t\theader = model.find(\"h1\")\n",
    "\t\tcontent = model.find(\"p\")\n",
    "\n",
    "\t\tif header is None or content is None:\n",
    "\t\t\tself.content = content\n",
    "\t\tif content:\n",
    "\t\t\tcontent = content.parent\n",
    "\t\telse:\n",
    "\n",
    "\t\t\tcontent = model.find('p')\n",
    "\t\t\tprint(f'article body not found: {content}')\n",
    "\n",
    "\t\tif header is None or content is None:\n",
    "\t\t\tself.article_text = \"\"\n",
    "\t\t\treturn\n",
    "\n",
    "\t\ttexts = header.findAll(string=True) + content.findAll(string=True)\n",
    "\t\tvisible_texts = filter(tag_visible, texts)\n",
    "\t\tself.article_text = \"\\n\".join(t.strip() for t in visible_texts)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HnJnucucHbrZ",
    "outputId": "a331f50f-b73a-4305-9db5-1bcffc3521b2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:08.662964Z",
     "start_time": "2024-09-17T14:55:07.130781Z"
    }
   },
   "source": [
    "doc = HtmlnbcnewsArticle(\n",
    "\t\"https://www.nbcnews.com/news/us-news/jeff-bezos-sells-nearly-12-million-amazon-shares-least-2-billion-come-rcna138274\")\n",
    "doc.download()\n",
    "\n",
    "doc.parse()\n",
    "print(doc.article_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff Bezos sells nearly 12 million Amazon shares worth at least $2 billion, with more to come\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjJxYtOqHbrZ"
   },
   "source": [
    "### Main class\n",
    "\n",
    "The main indexer logic is here. We organize it as a crawler generator that adds certain visited pages to inverted index and saves them on disk.\n",
    "\n",
    "- `crawl_generator_for_index` method crawles the given website doing BFS, starting from `source` within given `depth`. Considers only inner pages (starting with https://www.nbcnews.com/...) for visiting. To speed up, do not consider pages with content type other than `html`: `.pdf`, `.mp3`, `.avi`, `.mp4`, `.txt`, ... . If crawler encounters an article page (of a form https://www.nbcnews.com/world/...), it saves its content in a file in `collection_path` folder and populates the inverted index calling `index_doc` method. When done, saves on disk three resulting dictionaries:\n",
    "    - `doc_urls`: `{doc_id : url}`\n",
    "    - `index`: `{term : [collection_frequency, (doc_id_1, doc_freq_1), (doc_id_2, doc_freq_2), ...]}`\n",
    "    - `doc_lengths`: `{doc_id : doc_length}`\n",
    "\n",
    "    `limit` parameter is given for testing - if not `None`, break the loop when number of saved articles exceeds the `limit` and return without writing dictionaries to disk.\n",
    "    \n",
    "- `index_doc` method parses and preprocesses the content of a `doc` and adds it to the inverted index. Also keeps track of document lengths in a `doc_lengths` dictionary.\n",
    "\n",
    "Your crawler have to print visited urls as it runs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n9i_kLMlHbrZ",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:08.685747Z",
     "start_time": "2024-09-17T14:55:08.666964Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "from queue import Queue\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "class nbcnewsSpecificIndexer:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# dictionaries to populate\n",
    "\t\tself.doc_urls = {}\n",
    "\t\tself.index = {}\n",
    "\t\tself.doc_lengths = {}\n",
    "\t\t# preprocessor\n",
    "\t\tself.prep = Preprocessor()\n",
    "\n",
    "\tdef crawl_generator_for_index(self, source, depth, collection_path=\"collection\", limit=None):\n",
    "\n",
    "\t\tq = Queue()\n",
    "\t\tq.put((source, 0))\n",
    "\t\tvisited = set()\n",
    "\t\tdoc_counter = 0\n",
    "\t\t# creating a folder if needed\n",
    "\t\tif not os.path.exists(collection_path):\n",
    "\t\t\tos.makedirs(collection_path)\n",
    "\t\t# doing a BFS\n",
    "\t\twhile not q.empty():\n",
    "\t\t\turl, url_depth = q.get()\n",
    "\t\t\tif url not in visited:\n",
    "\t\t\t\tvisited.add(url)\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tdoc = HtmlnbcnewsArticle(url)  # download and parse url\n",
    "\t\t\t\t\tif doc.download():\n",
    "\t\t\t\t\t\tdoc.parse()\n",
    "\t\t\t\t\t\t### TODO write a regular expression that will match only webpages under nbcnews domain\n",
    "\t\t\t\t\t\t# if re.match(r\"^https?://[^/]+nbcnews[^/]+/.*$\", url):\n",
    "\t\t\t\t\t\tif re.match(r\"https?\\://www\\.nbcnews\\.com/\\w+/\\w+\", url):\n",
    "\n",
    "\t\t\t\t\t\t\t# if url.startswith(\"https://www.nbcnews.com/\"):\n",
    "\t\t\t\t\t\t\t# print(url)\n",
    "\t\t\t\t\t\t\tdoc.persist(collection_path)\n",
    "\t\t\t\t\t\t\tself.doc_urls[doc_counter] = url\n",
    "\t\t\t\t\t\t\tself.index_doc(doc, doc_counter)\n",
    "\t\t\t\t\t\t\tdoc_counter += 1\n",
    "\t\t\t\t\t\t\tyield doc\n",
    "\t\t\t\t\t\t\tif limit is not None and doc_counter == limit:\n",
    "\t\t\t\t\t\t\t\treturn\n",
    "\n",
    "\t\t\t\t\t\t\t# filter links, consider only inner html pages\n",
    "\t\t\t\t\t\tif url_depth + 1 < depth:\n",
    "\t\t\t\t\t\t\tvalid_anchors = filter(self.accepted_url, doc.anchors)\n",
    "\t\t\t\t\t\t\tfor a in valid_anchors:\n",
    "\t\t\t\t\t\t\t\tq.put((a[1], url_depth + 1))\n",
    "\n",
    "\t\t\t\texcept FileNotFoundError as e:\n",
    "\t\t\t\t\tprint(\"Analyzing\", url, \"led to FileNotFoundError\")\n",
    "\n",
    "\tdef accepted_url(self, anchor):\n",
    "\t\turl = str(anchor[1])\n",
    "\t\tif not url.startswith(\"https://www.nbcnews.com\"):\n",
    "\t\t\treturn False\n",
    "\t\tif url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\tdef index_doc(self, doc, doc_id):\n",
    "\t\t# add documents to index\n",
    "\t\tdoc.parse()\n",
    "\t\t# preprocess - tokenize, remove stopwords and non-alphanumeric tokens and stem\n",
    "\t\tcontent = self.prep.preprocess(doc.article_text)\n",
    "\t\tself.doc_lengths[doc_id] = len(content)\n",
    "\t\t# get dict of terms in current article\n",
    "\t\tarticle_index = Counter(content)\n",
    "\t\t# update global index\n",
    "\t\tfor term in article_index.keys():\n",
    "\t\t\tarticle_freq = article_index[term]\n",
    "\t\t\tif term not in self.index:\n",
    "\t\t\t\tself.index[term] = [article_freq, (doc_id, article_freq)]\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.index[term][0] += article_freq\n",
    "\t\t\t\tself.index[term].append((doc_id, article_freq))"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjECDhcmHbrZ"
   },
   "source": [
    "### Tests\n",
    "\n",
    "Please make sure your crawler prints out urls with `print(k, c.url)`"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\n",
    "\t'https://www.nbcnews.com/news/us-news/jeff-bezos-sells-nearly-12-million-amazon-shares-least-2-billion-come-rcna138274').status_code"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wJwF8gd0isX2",
    "outputId": "d0b626d7-7295-4d14-c797-7adc43269906",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:09.977723Z",
     "start_time": "2024-09-17T14:55:08.687789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2FRr1MFvHbrZ",
    "outputId": "cd9c2cd7-6889-47da-8225-31aea08a2394",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:53.811905Z",
     "start_time": "2024-09-17T14:55:09.980157Z"
    }
   },
   "source": [
    "indexer = nbcnewsSpecificIndexer()\n",
    "k = 1\n",
    "\n",
    "for c in indexer.crawl_generator_for_index(\n",
    "\tsource=\"https://www.nbcnews.com/news\",\n",
    "\tdepth=2,\n",
    "\tcollection_path=\"test_collection\",\n",
    "\tlimit=15):\n",
    "\tprint(k, c.url)\n",
    "\tk += 1\n",
    "\n",
    "\n",
    "# assert type(indexer.index) is dict\n",
    "# assert type(indexer.index['nbcnews']) is list\n",
    "# assert type(indexer.index['nbcnews'][0]) is int\n",
    "# assert type(indexer.index['nbcnews'][1]) is tuple"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438\n",
      "2 https://www.nbcnews.com/politics/2024-election/live-blog/trump-harris-presidential-election-live-updates-rcna171169\n",
      "3 https://www.nbcnews.com/news/us-news/live-blog/donald-trump-apparent-assassination-attempt-live-updates-rcna171416\n",
      "4 https://www.nbcnews.com/news/us-news/nbc-affiliates-n19981\n",
      "5 https://www.nbcnews.com/news/weather\n",
      "6 https://www.nbcnews.com/politics/2024-presidential-election\n",
      "7 https://www.nbcnews.com/news/nbc-news-now-live-audio-listen-live-news-audio-day-rcna70163\n",
      "8 https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55828\n",
      "9 https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55827\n",
      "10 https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55825\n",
      "11 https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55822\n",
      "12 https://www.nbcnews.com/now/video/sean-diddy-combs-charged-with-sex-trafficking-and-racketeering-219520581619\n",
      "13 https://www.nbcnews.com/news/us-news/former-danity-kane-member-sues-sean-combs-alleging-groped-threatened-rcna170617\n",
      "14 https://www.nbcnews.com/news/us-news/diddy-lawsuits-timeline-allegations-what-know-rcna145335\n",
      "15 https://www.nbcnews.com/news/world/hezbollah-pagers-expolsion-lebanon-handheld-devices-rcna171457\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHI7wDluHbra"
   },
   "source": [
    "Please test these documents contain a desired stem (or its derivate):"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "indexer.index.keys()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fw7WJ448jDZT",
    "outputId": "139a6b08-961c-4cd1-8345-a6f96394f7ee",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:53.819943Z",
     "start_time": "2024-09-17T14:55:53.813942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sean', 'comb', 'arrest', 'live', 'updat', 'charg', 'sex', 'traffick', 'racket', 'elect', 'trump', 'attend', 'town', 'hall', 'michigan', 'harri', 'speak', 'black', 'journalist', 'philadelphia', 'secret', 'servic', 'under', 'pressur', 'after', 'second', 'appar', 'assassin', 'attempt', 'nbc', 'affili', 'weather', 'news', 'now', 'audio', 'diddi', 'former', 'daniti', 'kane', 'member', 'sue', 'alleg', 'grope', 'threaten', 'her', 'timelin', 'indict', 'what', 'know', 'hezbollah', 'say', 'handheld', 'devic', 'explod', 'across', 'lebanon', 'dozen', 'report', 'injur'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yBP5FtJLHbra",
    "outputId": "27074b59-f157-44e1-d53e-f6dc5d5d5091",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:53.915526Z",
     "start_time": "2024-09-17T14:55:53.821939Z"
    }
   },
   "source": [
    "some_stem = 'live'\n",
    "print(indexer.index[some_stem])\n",
    "for pair in indexer.index[some_stem][1:]:\n",
    "\tprint(indexer.doc_urls[pair[0]])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, (0, 1), (1, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438\n",
      "https://www.nbcnews.com/politics/2024-election/live-blog/trump-harris-presidential-election-live-updates-rcna171169\n",
      "https://www.nbcnews.com/news/us-news/live-blog/donald-trump-apparent-assassination-attempt-live-updates-rcna171416\n",
      "https://www.nbcnews.com/news/nbc-news-now-live-audio-listen-live-news-audio-day-rcna70163\n",
      "https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55828\n",
      "https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55827\n",
      "https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55825\n",
      "https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55822\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ9BkdwLHbra"
   },
   "source": [
    "### 1.2.4. Building an index"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fe-vfrzNHbra",
    "outputId": "d561a200-c5d2-4d3c-8818-d9299b3c9c08",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.011767Z",
     "start_time": "2024-09-17T14:55:53.918922Z"
    }
   },
   "source": [
    "# indexer = nbcnewsSpecificIndexer()\n",
    "# for k, c in enumerate(\n",
    "# \tindexer\n",
    "# \t\t.crawl_generator_for_index(\n",
    "# \t\t\"https://www.nbcnews.com/\",\n",
    "# \t\t3,\n",
    "# \t\t\"docs_collection\",\n",
    "# \t\tlimit=10  # optional limit\n",
    "# \t)):\n",
    "# \tprint(k + 1, c.url)"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yO1CW_GtHbra"
   },
   "source": [
    "### Index statistics\n",
    "\n",
    "Load the index and print the statistics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iSQxbkczHbra",
    "outputId": "fcfe6de2-f011-4762-89e3-b8876642f78d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.111712Z",
     "start_time": "2024-09-17T14:55:54.019437Z"
    }
   },
   "source": [
    "print('Total index keys count', len(indexer.index))\n",
    "\n",
    "print('\\nTop stems by number of documents they apperared in:')\n",
    "sorted_by_n_docs = sorted(indexer.index.items(), key=lambda kv: (len(kv[1]), kv[0]), reverse=True)\n",
    "print([(sorted_by_n_docs[i][0], len(sorted_by_n_docs[i][1]) - 1) for i in range(20)])\n",
    "\n",
    "print('\\nTop stems by overall frequency:')\n",
    "sorted_by_freq = sorted(indexer.index.items(), key=lambda kv: (kv[1][0], kv[0]), reverse=True)\n",
    "print([(sorted_by_freq[i][0], sorted_by_freq[i][1][0]) for i in range(20)])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total index keys count 59\n",
      "\n",
      "Top stems by number of documents they apperared in:\n",
      "[('sean', 8), ('live', 8), ('comb', 8), ('updat', 7), ('traffick', 6), ('sex', 6), ('racket', 6), ('charg', 6), ('arrest', 6), ('trump', 2), ('nbc', 2), ('what', 1), ('weather', 1), ('under', 1), ('town', 1), ('timelin', 1), ('threaten', 1), ('sue', 1), ('speak', 1), ('servic', 1)]\n",
      "\n",
      "Top stems by overall frequency:\n",
      "[('sean', 8), ('live', 8), ('comb', 8), ('updat', 7), ('traffick', 6), ('sex', 6), ('racket', 6), ('charg', 6), ('arrest', 6), ('trump', 2), ('nbc', 2), ('what', 1), ('weather', 1), ('under', 1), ('town', 1), ('timelin', 1), ('threaten', 1), ('sue', 1), ('speak', 1), ('servic', 1)]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nik5EiNKHbra"
   },
   "source": [
    "## Answering a query (finally)\n",
    "\n",
    "Now, given that we already have built the inverted index, it's time to utilize it for answering user queries. In this class there are two methods you need to implement:\n",
    "- `boolean_retrieval`, the simplest form of document retrieval which returns a set of documents such that each one contains all query terms. Returns a set of document ids. Refer to *ch.1* of the book for details;\n",
    "- `okapi_scoring`, Okapi BM25 ranking function - assigns scores to documents in the collection that are relevant to the user query. Returns a dictionary of scores, `doc_id:score`. Read about it in [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) and implement accordingly.\n",
    "\n",
    "Both methods accept `query` parameter in a form of a dictionary, `term:frequency`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WAcmazWZHbra",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.205187Z",
     "start_time": "2024-09-17T14:55:54.114697Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "class QueryProcessing:\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef prepare_query(raw_query):\n",
    "\t\tprep = Preprocessor()\n",
    "\t\t# pre-process query the same way as documents\n",
    "\t\tquery = prep.preprocess(raw_query)\n",
    "\t\t# count frequency\n",
    "\t\treturn Counter(query)\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef boolean_retrieval(query, index):\n",
    "\n",
    "\t\t# retrieve a set of documents containing all query terms\n",
    "\t\t#TODO retrieve a set of documents containing all query terms\n",
    "\t\tRetDocs = []\n",
    "\t\tfor term in query:\n",
    "\t\t\tdocs = index.get(term)\n",
    "\t\t\tif not docs:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdocs = [k for k, v in docs[1:]]\n",
    "\t\t\tdocs = set(docs)\n",
    "\t\t\tRetDocs.append(docs)\n",
    "\t\t\n",
    "\t\tif not RetDocs:\n",
    "\t\t\treturn set()\n",
    "\n",
    "\t\tdocs = set.intersection(*RetDocs)\n",
    "\t\treturn docs\n",
    "\n",
    "\t\t# return {0, 1, 3}\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef okapi_scoring(query, doc_lengths, index, k1=1.2, b=0.75):\n",
    "\t\t#TODO retrieve relevant documents with scores\n",
    "\n",
    "\t\tscores = {}\n",
    "\t\tN = len(doc_lengths)\n",
    "\t\tavgdl = sum(doc_lengths.values()) / float(len(doc_lengths))\n",
    "\t\tfor term in query.keys():\n",
    "\t\t\tif term not in index:  # ignoring absent terms\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tn_docs = index[term][0]\n",
    "\t\t\tidf = math.log(1 + (N - n_docs + 0.5) / (n_docs + 0.5))\n",
    "\t\t\tpostings = index[term][1:]\n",
    "\t\t\tfor posting in postings:\n",
    "\t\t\t\tdoc_id = posting[0]\n",
    "\t\t\t\tdoc_tf = posting[1]\n",
    "\t\t\t\tscore = idf * (doc_tf * (k1 + 1)) / (doc_tf + k1 * (1 - b + b * avgdl))\n",
    "\t\t\t\tif doc_id not in scores:\n",
    "\t\t\t\t\tscores[doc_id] = score\n",
    "\t\t\t\telse:  # accumulate scores\n",
    "\t\t\t\t\tscores[doc_id] += score\n",
    "\t\treturn scores\n",
    "\t\t# return {0: 0.32, 5: 1.17}"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BimTi5vuHbra"
   },
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CLGC4LFkHbrb",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.321775Z",
     "start_time": "2024-09-17T14:55:54.207373Z"
    }
   },
   "source": [
    "test_doc_lengths = {1: 20, 2: 15, 3: 10, 4: 20, 5: 30}\n",
    "test_index = {'x': [2, (1, 1), (2, 1)], 'y': [2, (1, 1), (3, 1)], 'z': [3, (2, 1), (4, 2)]}\n",
    "\n",
    "test_query1 = QueryProcessing.prepare_query('x z')\n",
    "test_query2 = QueryProcessing.prepare_query('x y')\n",
    "\n",
    "assert QueryProcessing.boolean_retrieval(test_query1, test_index) == {2}\n",
    "assert QueryProcessing.boolean_retrieval(test_query2, test_index) == {1}\n",
    "# okapi_res = QueryProcessing.okapi_scoring(test_query2, test_doc_lengths, test_index)\n",
    "# assert all(k in okapi_res for k in (1, 2, 3))\n",
    "# assert not any(k in okapi_res for k in (4, 5))\n",
    "# assert okapi_res[1] > okapi_res[3] > okapi_res[2]"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3S9ZiU2Hbrb"
   },
   "source": [
    "### And now query the real index"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g5F7qDBuHbrb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e101e366-d3c3-4691-a5a2-b57c76ba621b",
    "ExecuteTime": {
     "end_time": "2024-09-17T14:56:41.964312Z",
     "start_time": "2024-09-17T14:56:41.952886Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "# queries = [\"russia missle\" , \"isreal palestine\", \"taylor swift\"]\n",
    "queries = ['skin care', 'trump arrested']\n",
    "for q in queries:\n",
    "\tprint(q)\n",
    "\tqobj = QueryProcessing.prepare_query(q)\n",
    "\tfor res in QueryProcessing.boolean_retrieval(qobj, indexer.index):\n",
    "\t\tprint('\\t', indexer.doc_urls[res])\n",
    "\n",
    "\ts = time.time()\n",
    "\tokapi_res = QueryProcessing.okapi_scoring(qobj, indexer.doc_lengths, indexer.index)\n",
    "\te = time.time()\n",
    "\tprint(f\"\\t == Okapi Time: {e - s:.5f} ==\")\n",
    "\tfor res in okapi_res:\n",
    "\t\tprint('\\t', indexer.doc_urls[res], okapi_res[res])\n",
    "\n",
    "\tprint()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skin care\n",
      "\t == Okapi Time: 0.00002 ==\n",
      "\n",
      "trump arrested\n",
      "\t == Okapi Time: 0.00003 ==\n",
      "\t https://www.nbcnews.com/politics/2024-election/live-blog/trump-harris-presidential-election-live-updates-rcna171169 0.5054276706441062\n",
      "\t https://www.nbcnews.com/news/us-news/live-blog/donald-trump-apparent-assassination-attempt-live-updates-rcna171416 0.5054276706441062\n",
      "\t https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438 0.24526366333465568\n",
      "\t https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55828 0.24526366333465568\n",
      "\t https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55827 0.24526366333465568\n",
      "\t https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55825 0.24526366333465568\n",
      "\t https://www.nbcnews.com/news/us-news/live-blog/sean-diddy-combs-arrest-live-updates-rcna171438#rcrd55822 0.24526366333465568\n",
      "\t https://www.nbcnews.com/news/us-news/diddy-lawsuits-timeline-allegations-what-know-rcna145335 0.24526366333465568\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.576942Z",
     "start_time": "2024-09-17T14:55:54.447528Z"
    }
   },
   "cell_type": "code",
   "source": "indexer.index",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sean': [8,\n",
       "  (0, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1)],\n",
       " 'comb': [8,\n",
       "  (0, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1)],\n",
       " 'arrest': [6, (0, 1), (7, 1), (8, 1), (9, 1), (10, 1), (13, 1)],\n",
       " 'live': [8, (0, 1), (1, 1), (2, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " 'updat': [7, (0, 1), (1, 1), (2, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " 'charg': [6, (0, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " 'sex': [6, (0, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " 'traffick': [6, (0, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " 'racket': [6, (0, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " 'elect': [1, (1, 1)],\n",
       " 'trump': [2, (1, 1), (2, 1)],\n",
       " 'attend': [1, (1, 1)],\n",
       " 'town': [1, (1, 1)],\n",
       " 'hall': [1, (1, 1)],\n",
       " 'michigan': [1, (1, 1)],\n",
       " 'harri': [1, (1, 1)],\n",
       " 'speak': [1, (1, 1)],\n",
       " 'black': [1, (1, 1)],\n",
       " 'journalist': [1, (1, 1)],\n",
       " 'philadelphia': [1, (1, 1)],\n",
       " 'secret': [1, (2, 1)],\n",
       " 'servic': [1, (2, 1)],\n",
       " 'under': [1, (2, 1)],\n",
       " 'pressur': [1, (2, 1)],\n",
       " 'after': [1, (2, 1)],\n",
       " 'second': [1, (2, 1)],\n",
       " 'appar': [1, (2, 1)],\n",
       " 'assassin': [1, (2, 1)],\n",
       " 'attempt': [1, (2, 1)],\n",
       " 'nbc': [2, (3, 1), (6, 1)],\n",
       " 'affili': [1, (3, 1)],\n",
       " 'weather': [1, (4, 1)],\n",
       " 'news': [1, (6, 1)],\n",
       " 'now': [1, (6, 1)],\n",
       " 'audio': [1, (6, 1)],\n",
       " 'diddi': [1, (11, 1)],\n",
       " 'former': [1, (12, 1)],\n",
       " 'daniti': [1, (12, 1)],\n",
       " 'kane': [1, (12, 1)],\n",
       " 'member': [1, (12, 1)],\n",
       " 'sue': [1, (12, 1)],\n",
       " 'alleg': [1, (12, 1)],\n",
       " 'grope': [1, (12, 1)],\n",
       " 'threaten': [1, (12, 1)],\n",
       " 'her': [1, (12, 1)],\n",
       " 'timelin': [1, (13, 1)],\n",
       " 'indict': [1, (13, 1)],\n",
       " 'what': [1, (13, 1)],\n",
       " 'know': [1, (13, 1)],\n",
       " 'hezbollah': [1, (14, 1)],\n",
       " 'say': [1, (14, 1)],\n",
       " 'handheld': [1, (14, 1)],\n",
       " 'devic': [1, (14, 1)],\n",
       " 'explod': [1, (14, 1)],\n",
       " 'across': [1, (14, 1)],\n",
       " 'lebanon': [1, (14, 1)],\n",
       " 'dozen': [1, (14, 1)],\n",
       " 'report': [1, (14, 1)],\n",
       " 'injur': [1, (14, 1)]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.696088Z",
     "start_time": "2024-09-17T14:55:54.579799Z"
    }
   },
   "cell_type": "code",
   "source": "indexer.index.get(\"care\")",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T14:55:54.703400Z",
     "start_time": "2024-09-17T14:55:54.698817Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
