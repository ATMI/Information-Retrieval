{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Submission Instructions and Format\n",
    "\n",
    "### 1. **Naming Convention for Submission**\n",
    "- Each student must submit their homework as a Jupyter Notebook (`.ipynb`) file.\n",
    "- The file must be named with your **email address**. For example, if your email is `j.doe@innopolis.university`, the file should be named `j.doe@innopolis.university.ipynb`.\n",
    "- Make sure that the notebook contains all necessary classes and functions outlined in the tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Structure of the Homework**\n",
    "\n",
    "The homework is divided into **four tasks**, and each task requires you to implement specific classes and functions. The tasks build upon concepts of web scraping, HTML parsing, document processing, and crawling.\n",
    "\n",
    "Here’s a breakdown of what is expected in each task:\n",
    "\n",
    "#### **Task 1: Artifact Caching System**\n",
    "- **Class**: `Artifact`\n",
    "  - Implement a class that can **download**, **store**, and **retrieve** digital content from a URL.\n",
    "  - This class should:\n",
    "    - Fetch content from a URL and store it in memory.\n",
    "    - Save the content to a local file to avoid redundant downloads.\n",
    "    - Retrieve the content from the local cache if it already exists.\n",
    "- **Methods**:\n",
    "  - `fetch_artifact()`: Downloads the content from the URL.\n",
    "  - `store_artifact()`: Stores the content locally in a unique file.\n",
    "  - `retrieve_artifact()`: Retrieves the content from the local cache.\n",
    "  \n",
    "---\n",
    "\n",
    "#### **Task 2: Smithsonian Snapshot Parser**\n",
    "- **Class**: `SmithsonianParser`\n",
    "  - Implement a class that parses HTML pages like [Smithsonian Snapshots](https://www.si.edu/newsdesk/snapshot/how-very-logical).\n",
    "  - This class should:\n",
    "    - Extract all links (`<a>` tags) and store them as a list of tuples.\n",
    "    - Extract all image URLs (`<img>` tags) and store them in a list.\n",
    "    - Clean the text from the page, removing unnecessary elements (scripts, styles).\n",
    "- **Methods**:\n",
    "  - `fetch_page()`: Downloads the HTML content of a page.\n",
    "  - `parse()`: Parses the content and extracts links, images, and cleaned text.\n",
    "  - `get_anchors()`, `get_images()`, and `get_text()`: Returns the extracted data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Task 3: Text Analysis of Smithsonian Snapshot**\n",
    "- **Class**: `SmithsonianTextAnalyzer`\n",
    "  - Implement a class that analyzes the text content of a Smithsonian Snapshot page.\n",
    "  - This class should:\n",
    "    - Perform **word frequency analysis**.\n",
    "    - **Segment sentences** and split text properly.\n",
    "    - Clean the text to remove special characters and whitespace.\n",
    "- **Methods**:\n",
    "  - `analyze()`: Fetches the page content, processes the text, and generates word frequency statistics.\n",
    "  - `get_word_stats()`: Returns word frequency in the form of a `Counter` object.\n",
    "  - `split_into_sentences()`: Splits the text into sentences.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Task 4: Smithsonian Snapshot Web Crawler**\n",
    "- **Class**: `SmithsonianCrawler`\n",
    "  - Implement a web crawler that starts at the [Smithsonian Snapshots](https://www.si.edu/newsdesk/snapshots) page and crawls through linked snapshot articles.\n",
    "  - This class should:\n",
    "    - Crawl pages to a specified depth.\n",
    "    - Extract links, images, and cleaned text from each page.\n",
    "    - Return the results as soon as the page is processed.\n",
    "- **Methods**:\n",
    "  - `crawl()`: Recursively visits pages starting from a given URL.\n",
    "  - `crawl_generator()`: Generates content as the crawler processes each page.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Grading Process**\n",
    "- Each homework will be graded using an automated grading system.\n",
    "- The grading system will dynamically import and execute your code to test if all the tasks are implemented correctly.\n",
    "  \n",
    "---\n",
    "\n",
    "### 4. **Total Grade Breakdown**\n",
    "- **Task 1**: Artifact Caching System (25 points)\n",
    "- **Task 2**: Smithsonian Snapshot Parser (25 points)\n",
    "- **Task 3**: Text Analysis (25 points)\n",
    "- **Task 4**: Web Crawler (25 points)\n",
    "- **Total**: 100 points\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Detailed Feedback**\n",
    "- Feedback will be provided with specific details on:\n",
    "  - **What worked**: Indicating which parts of the code were implemented correctly and passed the tests.\n",
    "  - **What needs improvement**: Highlighting which tests failed and what parts of the code may require debugging or further development.\n",
    "- The feedback will help you understand your performance in each task.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Submission Guidelines**\n",
    "- Ensure that your notebook is properly formatted and runs without errors.\n",
    "- Do not use any external libraries unless instructed.\n",
    "- Each function and class must follow the naming conventions provided in this document.\n",
    "- Submit your notebook on time. Late submissions may not be accepted.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Final Tips**\n",
    "- Test each task thoroughly before submission.\n",
    "- Ensure your notebook is readable and well-documented.\n",
    "- Make use of comments to explain your code wherever necessary.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Archiving Virtual Artifacts - Preserving a Digital Museum\n",
    "\n",
    "#### 1.0.1. Task Description\n",
    "Imagine you are a data archivist working to preserve artifacts from the **Smithsonian Institution's digital collection**. Your job is to download, store, and manage different types of data (such as images, videos, and documents) to ensure they can be accessed later without repeated downloads.\n",
    "\n",
    "Use the [Smithsonian Institution Collections](https://www.si.edu/snapshot) as your source of artifacts. You are tasked with building a caching system that can store downloaded files in a structured way and retrieve them as needed.\n",
    "\n",
    "#### Tasks:\n",
    "1. `fetch_artifact()`: Download content from the Smithsonian's collection page based on a provided URL. The method should return `True` if successful, or `False` if the download fails.\n",
    "2. `store_artifact()`: Save the content of the artifact (text, image, etc.) in a local file system. Each artifact must be stored in its own unique file based on its URL.\n",
    "3. `retrieve_artifact()`: Load an artifact from your local storage using its URL to ensure that content is cached correctly and avoid redundant downloads.\n",
    "\n",
    "#### Criteria for Success:\n",
    "- Different URLs must map to different files, even if they belong to the same domain.\n",
    "- Binary files (e.g., images) must be handled correctly without corruption.\n",
    "- Artifacts that are already stored locally should not be downloaded again.\n",
    "\n",
    "#### Link: [Smithsonian Institution Collections](https://www.si.edu/newsdesk/snapshot/what-good-boy)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "\n",
    "\n",
    "class Artifact:\n",
    "\tdef __init__(self, url):\n",
    "\t\tself.url = url\n",
    "\t\tself.content = None\n",
    "\t\tself.filename = None\n",
    "\n",
    "\tdef generate_filename(self):\n",
    "\t\t\"\"\"\n",
    "\t\tGenerates a unique and safe filename based on the URL.\n",
    "\t\tYou will need to use a hash function (hint: hashlib).\n",
    "\t\t\"\"\"\n",
    "\t\tfilename = self.url.encode(\"utf-8\")\n",
    "\t\tfilename = hashlib.sha256(filename, usedforsecurity=False)\n",
    "\t\tfilename = filename.hexdigest() + \".bin\"\n",
    "\t\tself.filename = filename\n",
    "\n",
    "\tdef fetch_artifact(self):\n",
    "\t\t\"\"\"\n",
    "\t\tDownload the artifact from the given URL and store its content in memory.\n",
    "\t\tIf the download is successful, return True. Otherwise, return False.\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tresponse = requests.get(self.url)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\t\t\tself.content = response.content\n",
    "\t\texcept Exception:\n",
    "\t\t\tself.content = None\n",
    "\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\tdef store_artifact(self, directory=\"artifact_cache\"):\n",
    "\t\t\"\"\"\n",
    "\t\tStore the artifact content in a local file in a cache directory.\n",
    "\t\tEnsure the file is stored with a unique name to avoid overwriting.\n",
    "\t\t\"\"\"\n",
    "\t\tif self.url is None or self.content is None:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\tif self.filename is None:\n",
    "\t\t\tself.generate_filename()\n",
    "\n",
    "\t\tpath = os.path.join(directory, self.filename)\n",
    "\t\tif os.path.exists(path):\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tos.makedirs(directory, exist_ok=True)\n",
    "\t\t\twith open(path, \"wb\") as f:\n",
    "\t\t\t\tf.write(self.content)\n",
    "\t\texcept:\n",
    "\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\tdef retrieve_artifact(self, directory=\"artifact_cache\"):\n",
    "\t\t\"\"\"\n",
    "\t\tRetrieve the artifact from the local cache if it has been stored before.\n",
    "\t\tReturn True if successful, False otherwise.\n",
    "\t\t\"\"\"\n",
    "\t\tif self.url is None:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\tif self.filename is None:\n",
    "\t\t\tself.generate_filename()\n",
    "\n",
    "\t\tpath = os.path.join(directory, self.filename)\n",
    "\t\tif not os.path.exists(path):\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\twith open(path, \"rb\") as f:\n",
    "\t\t\t\tself.content = f.read()\n",
    "\t\texcept Exception:\n",
    "\t\t\treturn False\n",
    "\t\treturn True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Parsing Web Pages - Smithsonian Snapshot\n",
    "\n",
    "#### 2.0.2. Task Description\n",
    "For this task, you will be working with pages from the [Smithsonian Newsdesk Snapshot](https://www.si.edu/newsdesk/snapshot/how-very-logical). Your goal is to extract meaningful content such as links, images, and clean text from the page.\n",
    "\n",
    "You will need to:\n",
    "1. Extract all hyperlinks (anchor tags) from the page and store them as a list of tuples `('link_text', 'absolute_url')`. Make sure to handle relative links by converting them to absolute URLs.\n",
    "2. Collect all image URLs in a list. Ensure relative URLs are converted to absolute URLs.\n",
    "3. Extract the plain text from the page, ignoring scripts, styles, and comments.\n",
    "\n",
    "#### Criteria for Success:\n",
    "- Extract all links as `('link_text', 'absolute_url')` and handle relative URLs.\n",
    "- Extract all image URLs as absolute URLs.\n",
    "- Clean and extract the main text from the document.\n",
    "\n",
    "#### Link: [How Very Logical](https://www.si.edu/newsdesk/snapshot/how-very-logical)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class SmithsonianParser:\n",
    "\tdef __init__(self, url):\n",
    "\t\tself.url = url\n",
    "\t\tself.anchors = []\n",
    "\t\tself.images = []\n",
    "\t\tself.text = \"\"\n",
    "\n",
    "\tdef fetch_page(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFetch the HTML content of the given URL. If the request is successful,\n",
    "\t\treturn the page content; otherwise, return None.\n",
    "\t\t\"\"\"\n",
    "\t\twebpage = Artifact(self.url)\n",
    "\t\tif not webpage.retrieve_artifact():\n",
    "\t\t\twebpage.fetch_artifact()\n",
    "\t\treturn webpage.content\n",
    "\n",
    "\tdef parse(self, html_content):\n",
    "\t\t\"\"\"\n",
    "\t\tParse the HTML content using BeautifulSoup. You need to:\n",
    "\t\t1. Extract all anchor tags and store them as ('link_text', 'absolute_url').\n",
    "\t\t2. Extract all image URLs and store them in a list.\n",
    "\t\t3. Extract clean, readable text from the page.\n",
    "\t\t\"\"\"\n",
    "\t\tself.anchors = []\n",
    "\t\tself.images = []\n",
    "\n",
    "\t\tsoup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\t\t# for script in soup([\"script\", \"style\"]):\n",
    "\t\t# \tscript.extract()\n",
    "\n",
    "\t\tfor anchor in soup.find_all(\"a\"):\n",
    "\t\t\tlink_text = anchor.get_text(strip=True, separator=\" \")\n",
    "\t\t\tlink_url = anchor.get(\"href\")\n",
    "\t\t\tif link_text is None and link_url is None:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlink_url = urljoin(self.url, link_url)\n",
    "\t\t\tself.anchors.append((link_text, link_url))\n",
    "\n",
    "\t\tfor image in soup.findAll(\"img\"):\n",
    "\t\t\timage_url = image.get(\"src\")\n",
    "\t\t\tif image_url is None:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\timage_url = urljoin(self.url, image_url)\n",
    "\t\t\tself.images.append(image_url)\n",
    "\n",
    "\t\tself.text = soup.get_text(strip=True, separator=\" \")\n",
    "\n",
    "\tdef get_anchors(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturn the list of anchors extracted from the page.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.anchors\n",
    "\n",
    "\tdef get_images(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturn the list of image URLs extracted from the page.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.images\n",
    "\n",
    "\tdef get_text(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturn the cleaned text content extracted from the page.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Summarizing Smithsonian Snapshots\n",
    "\n",
    "#### 3.0.3. Task Description\n",
    "You will analyze the text content from one of the Smithsonian Snapshot pages, such as [How Very Logical](https://www.si.edu/newsdesk/snapshot/how-very-logical). Your task is to extract and analyze the text content from this page, focusing on the following:\n",
    "\n",
    "1. **Extract key phrases**: Use basic natural language processing (NLP) techniques to identify the most frequently used words and key phrases from the main body of the text.\n",
    "2. **Sentence segmentation**: Split the text into individual sentences, making sure to handle punctuation and proper sentence breaks appropriately.\n",
    "3. **Clean the text**: Remove any extraneous characters, symbols, or whitespace.\n",
    "\n",
    "### Tasks:\n",
    "1. **Word Frequency Analysis**: Implement a method to count the frequency of each word in the page content, converting all words to lowercase.\n",
    "2. **Sentence Splitting**: Implement a method to split the content into individual sentences, being mindful of punctuation and line breaks.\n",
    "3. **Cleaning and Normalization**: Clean the text to remove any special characters or unnecessary whitespace.\n",
    "\n",
    "#### Criteria for Success:\n",
    "- The `get_word_stats()` method should return a frequency distribution of words as a `Counter` object.\n",
    "- Sentences should be extracted cleanly from the page’s main text.\n",
    "- The text should be normalized (lowercased, and special characters should be removed).\n",
    "\n",
    "#### Link: [How Very Logical](https://www.si.edu/newsdesk/snapshot/how-very-logical)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class SmithsonianTextAnalyzer:\n",
    "\tdef __init__(self, url):\n",
    "\t\tself.url = url\n",
    "\t\tself.text = \"\"\n",
    "\t\tself.sentences = []\n",
    "\t\tself.word_frequency = None\n",
    "\n",
    "\tdef fetch_page(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFetch the HTML content of the Smithsonian Snapshot page.\n",
    "\t\tReturn the page content if successful, else return None.\n",
    "\t\t\"\"\"\n",
    "\t\twebpage = Artifact(self.url)\n",
    "\t\tif not webpage.retrieve_artifact():\n",
    "\t\t\twebpage.fetch_artifact()\n",
    "\t\treturn webpage.content\n",
    "\n",
    "\tdef clean_text(self, html_content):\n",
    "\t\t\"\"\"\n",
    "\t\tUse BeautifulSoup to extract clean text from the HTML content.\n",
    "\t\tRemove scripts, styles, and special characters.\n",
    "\t\t\"\"\"\n",
    "\t\tsoup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\t\tfor script in soup([\"script\", \"style\"]):\n",
    "\t\t\tscript.extract()\n",
    "\n",
    "\t\tself.text = soup.get_text(separator=\" \")\n",
    "\t\tself.text = self.text.lower()\n",
    "\n",
    "\tdef split_into_sentences(self):\n",
    "\t\t\"\"\"\n",
    "\t\tUse nltk's sentence tokenizer to split the cleaned text into sentences.\n",
    "\t\t\"\"\"\n",
    "\t\tself.sentences = []\n",
    "\t\tif not self.text:\n",
    "\t\t\treturn\n",
    "\t\t\n",
    "\t\tdef clean_sentence(text):\n",
    "\t\t\ttext = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove all chars except dot, spaces, letters and digits\n",
    "\t\t\ttext = re.sub(r\"\\s+\", \" \", text)  # remove multiple spaces\n",
    "\t\t\ttext = text.strip()\n",
    "\t\t\treturn text\n",
    "\n",
    "\t\tself.sentences = nltk.tokenize.sent_tokenize(self.text)\n",
    "\t\tself.sentences = [clean_sentence(sentence) for sentence in self.sentences]\n",
    "\n",
    "\tdef get_word_stats(self):\n",
    "\t\t\"\"\"\n",
    "\t\tCount the frequency of each word in the text. Return a Counter object.\n",
    "\t\tEnsure the text is lowercased for accurate counting.\n",
    "\t\t\"\"\"\n",
    "\t\t# Your code here\n",
    "\n",
    "\t\ttext = re.sub(r\"\\W\", \" \", self.text)\n",
    "\t\ttext = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "\t\twords = nltk.word_tokenize(text)\n",
    "\t\tself.word_frequency = Counter(words)\n",
    "\t\treturn self.word_frequency\n",
    "\n",
    "\tdef analyze(self):\n",
    "\t\t\"\"\"\n",
    "\t\tOrchestrate the fetching, cleaning, and analysis of the text from the page.\n",
    "\t\t- Fetch the HTML content.\n",
    "\t\t- Clean the text.\n",
    "\t\t- Split into sentences.\n",
    "\t\t- Get word frequency statistics.\n",
    "\t\t\"\"\"\n",
    "\t\t# Your code here\n",
    "\t\thtml_content = self.fetch_page()\n",
    "\t\tif not html_content:\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tself.clean_text(html_content)\n",
    "\t\tself.split_into_sentences()\n",
    "\t\tself.get_word_stats()\n",
    "\t\treturn self.word_frequency"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Building a Smithsonian Snapshots Crawler\n",
    "\n",
    "#### 4.0.4. Task Description\n",
    "In this task, you will create a **web crawler** that will start at the Smithsonian Snapshots page and follow links to gather and analyze the content from multiple snapshot pages. The Smithsonian Snapshot section contains multiple articles, and your crawler will explore these articles, download their content, and process the information.\n",
    "\n",
    "You will implement a web crawler that:\n",
    "1. Starts at the [Smithsonian Snapshots Page](https://www.si.edu/snapshot).\n",
    "2. Crawls through snapshot pages, extracting key information (links, images, and text) from each page.\n",
    "3. Follows links from the initial page to other snapshot articles up to a specified depth.\n",
    "4. Processes and stores the content from each crawled page.\n",
    "\n",
    "### Tasks:\n",
    "1. **Implement a Crawler**: Start crawling from the [Smithsonian Snapshots Page](https://www.si.edu/newsdesk/snapshots), gather links to snapshot articles, and visit each article.\n",
    "2. **Content Extraction**: For each visited page, extract:\n",
    "   - Anchor tags (`'link_text', 'absolute_url'`).\n",
    "   - Image URLs (absolute URLs).\n",
    "   - Cleaned text content from the body of the article.\n",
    "3. **Depth Control**: Implement a parameter to control the depth of the crawl (i.e., how many levels of links the crawler should follow).\n",
    "4. **Yield Results**: Your crawler should return a **generator** that yields the results (text, links, images) as soon as a page is processed, rather than collecting everything before returning.\n",
    "\n",
    "### Criteria for Success:\n",
    "- The crawler should respect the specified depth and only crawl the specified number of levels.\n",
    "- Each snapshot page should have its content (links, images, text) extracted and returned.\n",
    "- The crawler should handle relative links and convert them to absolute URLs.\n",
    "- The content should be cleaned and stored properly.\n",
    "\n",
    "#### Link: [Smithsonian Snapshots Page](https://www.si.edu/snapshot)\n",
    "\n",
    "# Make sure the total number of visited links doesn't exceed 10 links, or you might get 0 for the whole assignment due to long runtime when checking the links!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "class SmithsonianCrawler:\n",
    "\tdef __init__(self, start_url, max_depth=2):\n",
    "\t\tself.start_url = start_url\n",
    "\t\tself.max_depth = max_depth\n",
    "\t\tself.visited = set()\n",
    "\n",
    "\tdef fetch_page(self, url):\n",
    "\t\t\"\"\"\n",
    "\t\tFetch the HTML content from the given URL.\n",
    "\t\tReturn the page content if successful, else return None.\n",
    "\t\t\"\"\"\n",
    "\t\twebpage = Artifact(url)\n",
    "\t\tif not webpage.retrieve_artifact():\n",
    "\t\t\twebpage.fetch_artifact()\n",
    "\t\treturn webpage.content\n",
    "\n",
    "\tdef extract_content(self, html_content, base_url):\n",
    "\t\t\"\"\"\n",
    "\t\tExtract links, images, and clean text content from the page using BeautifulSoup.\n",
    "\t\tHandle relative URLs appropriately.\n",
    "\t\t\"\"\"\n",
    "\t\tparser = SmithsonianParser(base_url)\n",
    "\t\tparser.parse(html_content)\n",
    "\t\tcontent = {\n",
    "\t\t\t\"anchors\": parser.get_anchors(),\n",
    "\t\t\t\"images\": parser.get_images(),\n",
    "\t\t\t\"text\": parser.get_text()\n",
    "\t\t}\n",
    "\n",
    "\t\treturn content\n",
    "\n",
    "\t# Your code here\n",
    "\n",
    "\tdef crawl(self, url, depth=0, limit=None):\n",
    "\t\t\"\"\"\n",
    "\t\tRecursively crawl through pages starting from the given URL up to a specified depth.\n",
    "\t\tYou should follow links and process the page content.\n",
    "\t\t\"\"\"\n",
    "\t\tif (url in self.visited) or (limit and len(self.visited) >= limit):\n",
    "\t\t\treturn\n",
    "\n",
    "\t\twebpage = Artifact(url)\n",
    "\t\tif not webpage.retrieve_artifact():\n",
    "\t\t\tif webpage.fetch_artifact():\n",
    "\t\t\t\twebpage.store_artifact()\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn\n",
    "\n",
    "\t\tself.visited.add(url)\n",
    "\t\tcontent = self.extract_content(webpage.content, url)\n",
    "\t\tcontent[\"url\"] = url\n",
    "\t\tyield content\n",
    "\n",
    "\t\tfor anchor in content[\"anchors\"]:\n",
    "\t\t\t_, anchor_url = anchor\n",
    "\t\t\tyield from self.crawl(anchor_url, depth + 1, limit)\n",
    "\n",
    "\tdef crawl_generator(self):\n",
    "\t\t\"\"\"\n",
    "\t\tA generator that yields the extracted content of each crawled page as soon as it's processed.\n",
    "\t\t\"\"\"\n",
    "\t\tyield from self.crawl(self.start_url)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
